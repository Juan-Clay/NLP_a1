{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRTTeWrTnwTCCrzjs0HE8E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juan-Clay/NLP_a1/blob/main/assignment1final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce8faVx9OcXB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from itertools import product\n",
        "import random\n"
      ],
      "metadata": {
        "id": "kfYc1MuBZ_nW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "BKPyprSIakJn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "print(\"Done1\")"
      ],
      "metadata": {
        "id": "96nEFlGcaqTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92c016a-2b95-4ae1-9fab-bbc1e34768f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Done1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts (e.g., movie reviews).\n",
        "        num_words (int or None): Maximum number of tokens to keep (based on frequency).\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing binary presence of characters.\n",
        "    \"\"\"\n",
        "    # Use texts_to_matrix with mode 'binary' to create fixed-length vectors.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]"
      ],
      "metadata": {
        "id": "XCwxZ7rgfndo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MthdXI4pjIVe",
        "outputId": "b025cc03-f28f-4e67-e8ae-41726fcd6e05"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_char = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer_char.word_index) + 1)\n",
        "char_vocab_size = len(tokenizer_char.word_index) + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjttiFv6p0YL",
        "outputId": "60b13ecb-d570-454b-adc3-1932e77ea16c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocabulary size: 134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_seq_lengths = [len(seq) for seq in tokenizer_char.texts_to_sequences(train_texts)]\n",
        "print(f\"Average Character-Level Sequence Length: {np.mean(char_seq_lengths):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x05WZJoKOrCU",
        "outputId": "43688c41-3e17-447b-fb3c-d5b3ed28938f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Character-Level Sequence Length: 1326.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer_char, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer_char, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer_char, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n"
      ],
      "metadata": {
        "id": "p-_IeylytqG0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output"
      ],
      "metadata": {
        "id": "rztDu4Nfv3A-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ6Xcvqgv4je",
        "outputId": "702cad2e-ea88-47be-ddcc-1977b6f038ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6708 | Val Loss: 0.6625 | Accuracy: 0.6102 | Precision: 0.5880 | Recall: 0.6547\n",
            "Epoch 02 | Training Loss: 0.6613 | Val Loss: 0.6622 | Accuracy: 0.6034 | Precision: 0.5799 | Recall: 0.6601\n",
            "Epoch 03 | Training Loss: 0.6601 | Val Loss: 0.6603 | Accuracy: 0.6182 | Precision: 0.5885 | Recall: 0.7067\n",
            "Epoch 04 | Training Loss: 0.6569 | Val Loss: 0.6589 | Accuracy: 0.6104 | Precision: 0.5863 | Recall: 0.6671\n",
            "Epoch 05 | Training Loss: 0.6575 | Val Loss: 0.6611 | Accuracy: 0.6080 | Precision: 0.6147 | Recall: 0.5128\n",
            "Epoch 06 | Training Loss: 0.6533 | Val Loss: 0.6605 | Accuracy: 0.6016 | Precision: 0.5721 | Recall: 0.7067\n",
            "Epoch 07 | Training Loss: 0.6509 | Val Loss: 0.6568 | Accuracy: 0.6122 | Precision: 0.5966 | Recall: 0.6180\n",
            "Epoch 08 | Training Loss: 0.6468 | Val Loss: 0.6565 | Accuracy: 0.6102 | Precision: 0.5877 | Recall: 0.6568\n",
            "Epoch 09 | Training Loss: 0.6446 | Val Loss: 0.6584 | Accuracy: 0.6104 | Precision: 0.5985 | Recall: 0.5965\n",
            "Epoch 10 | Training Loss: 0.6420 | Val Loss: 0.6574 | Accuracy: 0.6098 | Precision: 0.5880 | Recall: 0.6518\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6581 | Test Accuracy: 0.6062 | Test Precision: 0.5983 | Test Recall: 0.6464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Changing to Word-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def word_level_tokenizer(texts, num_words=10000):  # Limit vocabulary to 10,000 words\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def word_texts_to_bow(tokenizer, texts):\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='count')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]"
      ],
      "metadata": {
        "id": "wqLL7Cq-xL5q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_word = word_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer_word.word_index) + 1)\n",
        "word_vocab_size = len(tokenizer_word.word_index) + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZ63Zr625oS",
        "outputId": "f3740d1f-e8f5-4955-dfe1-795fab8ee351"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocabulary size: 80169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_seq_lengths = [len(seq) for seq in tokenizer_word.texts_to_sequences(train_texts)]\n",
        "print(f\"Average Word-Level Sequence Length: {np.mean(word_seq_lengths):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvQUPQMsO9pN",
        "outputId": "785b892b-8936-4723-a8d2-d559020d7bff"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Word-Level Sequence Length: 224.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = word_texts_to_bow(tokenizer_word, train_texts)\n",
        "X_val   = word_texts_to_bow(tokenizer_word, val_texts)\n",
        "X_test  = word_texts_to_bow(tokenizer_word, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n"
      ],
      "metadata": {
        "id": "UcAtDwch9wNc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New input size:\", X_train.shape[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4WBBVw0_zwC",
        "outputId": "86b6c2f5-c8ea-44d5-cb53-c3d72acae8b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New input size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)  #adaptive optimization\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-LBSK95-ac3",
        "outputId": "a83a5cfb-5dc8-42da-eb8f-69d02510ea93"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.4464 | Val Loss: 0.3479 | Accuracy: 0.8596 | Precision: 0.9159 | Recall: 0.7822\n",
            "Epoch 02 | Training Loss: 0.2003 | Val Loss: 0.3259 | Accuracy: 0.8758 | Precision: 0.8303 | Recall: 0.9348\n",
            "Epoch 03 | Training Loss: 0.0914 | Val Loss: 0.3731 | Accuracy: 0.8816 | Precision: 0.8779 | Recall: 0.8779\n",
            "Epoch 04 | Training Loss: 0.0301 | Val Loss: 0.4905 | Accuracy: 0.8744 | Precision: 0.8748 | Recall: 0.8647\n",
            "Epoch 05 | Training Loss: 0.0080 | Val Loss: 0.6071 | Accuracy: 0.8784 | Precision: 0.8709 | Recall: 0.8795\n",
            "Epoch 06 | Training Loss: 0.0021 | Val Loss: 0.6954 | Accuracy: 0.8762 | Precision: 0.8609 | Recall: 0.8882\n",
            "Epoch 07 | Training Loss: 0.0010 | Val Loss: 0.7549 | Accuracy: 0.8750 | Precision: 0.8554 | Recall: 0.8932\n",
            "Epoch 08 | Training Loss: 0.0006 | Val Loss: 0.7950 | Accuracy: 0.8756 | Precision: 0.8556 | Recall: 0.8944\n",
            "Epoch 09 | Training Loss: 0.0004 | Val Loss: 0.8246 | Accuracy: 0.8762 | Precision: 0.8641 | Recall: 0.8837\n",
            "Epoch 10 | Training Loss: 0.0003 | Val Loss: 0.8581 | Accuracy: 0.8756 | Precision: 0.8573 | Recall: 0.8919\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.8927 | Test Accuracy: 0.8626 | Test Precision: 0.8663 | Test Recall: 0.8575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Comparison of Tokenization Approaches:\\n\")\n",
        "print(f\"Character-Level Vocabulary Size: {char_vocab_size}\")\n",
        "print(f\"Word-Level Vocabulary Size: {word_vocab_size}\")\n",
        "print(f\"Average Character-Level Sequence Length: {np.mean(char_seq_lengths):.2f}\")\n",
        "print(f\"Average Word-Level Sequence Length: {np.mean(word_seq_lengths):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p6PJJ6OPG4R",
        "outputId": "df3c4d65-cc5b-4375-c07f-529e9bbfebfd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Tokenization Approaches:\n",
            "\n",
            "Character-Level Vocabulary Size: 134\n",
            "Word-Level Vocabulary Size: 80169\n",
            "Average Character-Level Sequence Length: 1326.28\n",
            "Average Word-Level Sequence Length: 224.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(char_seq_lengths, bins=10, alpha=0.5, label='Character-Level Sequences', color='blue')\n",
        "plt.hist(word_seq_lengths, bins=10, alpha=0.5, label='Word-Level Sequences', color='green')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.title('Comparison of Character vs. Word-Level Tokenization')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "z-W5TA2lPNei",
        "outputId": "76b15ccd-b4dd-461d-d12d-3e83392d280d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Develop a model (or models) that can handle both tokenization types. Include the following adjustable hyper-parameters:"
      ],
      "metadata": {
        "id": "hoBne2wXPQrL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size,\n",
        "                 embedding_dim=128,\n",
        "                 num_hidden_layers=2,\n",
        "                 hidden_size=64,\n",
        "                 activation='relu',\n",
        "                 optimizer_choice='adam',\n",
        "                 learning_rate=0.001):\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Embedding layer\n",
        "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True))\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "\n",
        "    #Hidden layers\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(tf.keras.layers.Dense(hidden_size, activation=activation))\n",
        "\n",
        "    # Output Layer with 2 neurons for one-hot encoding\n",
        "    model.add(tf.keras.layers.Dense(2, activation='softmax'))  # 2 neurons for binary classification\n",
        "\n",
        "    # Choose optimizr\n",
        "    if optimizer_choice.lower() == 'adam':\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_choice.lower() == 'sgd':\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer_choice.lower() == 'rmsprop':\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer. Choose from ['adam', 'sgd', 'rmsprop']\")\n",
        "\n",
        "    #use categorical_crossentropy for one-hot encoding\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4PqFO6BiQF1c"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(vocab_size=5000, num_hidden_layers=2, hidden_size=128, activation='relu', optimizer_choice='adam')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTvgykK3QL2w",
        "outputId": "d65d7d74-91b1-433a-83dc-4084adc8d735"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.5007 - loss: 0.6938 - val_accuracy: 0.4945 - val_loss: 0.6932\n",
            "Epoch 2/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.5009 - loss: 0.6933 - val_accuracy: 0.5055 - val_loss: 0.6931\n",
            "Epoch 3/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.4919 - loss: 0.6933 - val_accuracy: 0.4945 - val_loss: 0.6933\n",
            "Epoch 4/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.5008 - loss: 0.6933 - val_accuracy: 0.5055 - val_loss: 0.6931\n",
            "Epoch 5/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5026 - loss: 0.6932 - val_accuracy: 0.5055 - val_loss: 0.6931\n",
            "Epoch 6/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.5018 - loss: 0.6932 - val_accuracy: 0.5055 - val_loss: 0.6931\n",
            "Epoch 7/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.5021 - loss: 0.6932 - val_accuracy: 0.5055 - val_loss: 0.6931\n",
            "Epoch 8/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.5021 - loss: 0.6932 - val_accuracy: 0.5055 - val_loss: 0.6931\n",
            "Epoch 9/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5021 - loss: 0.6932 - val_accuracy: 0.5055 - val_loss: 0.6931\n",
            "Epoch 10/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.5016 - loss: 0.6932 - val_accuracy: 0.5055 - val_loss: 0.6931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"learning_rate\": [0.001, 0.0005, 0.0001],\n",
        "    \"hidden_layers\": [1, 2, 3],\n",
        "    \"hidden_size\": [128, 256, 512],\n",
        "    \"batch_size\": [32, 64, 128],\n",
        "    \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"],\n",
        "    \"activation\": [\"relu\", \"tanh\", \"leaky_relu\"]\n",
        "}\n",
        "\n",
        "param_combinations = list(product(*param_grid.values()))"
      ],
      "metadata": {
        "id": "FP1__0ofROwn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(config, num_trials=3):\n",
        "\n",
        "    learning_rate = config[\"learning_rate\"]\n",
        "    hidden_layers = config[\"hidden_layers\"]\n",
        "    hidden_size = config[\"hidden_size\"]\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    optimizer_choice = config[\"optimizer\"]\n",
        "    activation = config[\"activation\"]\n",
        "\n",
        "    accuracies, losses = [], []\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Create model\n",
        "        model = create_model(vocab_size=5000,\n",
        "                             embedding_dim=128,\n",
        "                             num_hidden_layers=hidden_layers,\n",
        "                             hidden_size=hidden_size,\n",
        "                             activation=activation,\n",
        "                             optimizer_choice=optimizer_choice,\n",
        "                             learning_rate=learning_rate)\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            epochs=5,  # Reduce epochs for faster experimentation\n",
        "                            batch_size=batch_size,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            verbose=0)  # Suppress output\n",
        "\n",
        "        # Get last epoch results\n",
        "        final_loss, final_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "        losses.append(final_loss)\n",
        "        accuracies.append(final_acc)\n",
        "\n",
        "    return {\n",
        "        \"config\": config,\n",
        "        \"avg_loss\": np.mean(losses),\n",
        "        \"avg_accuracy\": np.mean(accuracies)\n",
        "    }"
      ],
      "metadata": {
        "id": "Oz2E5drQTsym"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_configs = random.sample(param_combinations, 50)\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in random_configs:\n",
        "    config_dict = dict(zip(param_grid.keys(), config))\n",
        "    print(f\"Testing configuration: {config_dict}\")\n",
        "    result = run_experiment(config_dict)\n",
        "    results.append(result)\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.to_csv(\"hyperparameter_results_random.csv\", index=False)\n",
        "print(\"Saved results to hyperparameter_results_random.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnW5vJpYTuqj",
        "outputId": "2192d024-2762-40da-cd1e-7b8e899c255d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 3, 'hidden_size': 128, 'batch_size': 128, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 3, 'hidden_size': 512, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 128, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 64, 'optimizer': 'sgd', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 128, 'batch_size': 32, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 128, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 256, 'batch_size': 64, 'optimizer': 'sgd', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 1, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'adam', 'activation': 'tanh'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 3, 'hidden_size': 256, 'batch_size': 128, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 64, 'optimizer': 'rmsprop', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 2, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'sgd', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 256, 'batch_size': 128, 'optimizer': 'adam', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 3, 'hidden_size': 512, 'batch_size': 128, 'optimizer': 'sgd', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 128, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 3, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 128, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 128, 'batch_size': 32, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 256, 'batch_size': 64, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 2, 'hidden_size': 128, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 2, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 1, 'hidden_size': 128, 'batch_size': 64, 'optimizer': 'rmsprop', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 2, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 3, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 3, 'hidden_size': 256, 'batch_size': 64, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 512, 'batch_size': 128, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 3, 'hidden_size': 128, 'batch_size': 64, 'optimizer': 'sgd', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 128, 'batch_size': 128, 'optimizer': 'sgd', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 256, 'batch_size': 128, 'optimizer': 'sgd', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 32, 'optimizer': 'sgd', 'activation': 'tanh'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'tanh'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 128, 'batch_size': 128, 'optimizer': 'sgd', 'activation': 'tanh'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'sgd', 'activation': 'tanh'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 1, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 2, 'hidden_size': 128, 'batch_size': 128, 'optimizer': 'rmsprop', 'activation': 'tanh'}\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_layers': 3, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 1, 'hidden_size': 512, 'batch_size': 32, 'optimizer': 'adam', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 512, 'batch_size': 128, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'adam', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 128, 'optimizer': 'adam', 'activation': 'tanh'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 1, 'hidden_size': 128, 'batch_size': 64, 'optimizer': 'rmsprop', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 3, 'hidden_size': 128, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation': 'relu'}\n",
            "Testing configuration: {'learning_rate': 0.0005, 'hidden_layers': 2, 'hidden_size': 512, 'batch_size': 64, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Testing configuration: {'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 256, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation': 'leaky_relu'}\n",
            "Saved results to hyperparameter_results_random.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest performing configuration had:"
      ],
      "metadata": {
        "id": "m1tf0FTxDCWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'learning_rate': 0.0001, 'hidden_layers': 1, 'hidden_size': 128, 'batch_size': 32, 'optimizer': 'sgd', 'activation': 'leaky_relu'}\tWith about 50 percent accuracy"
      ],
      "metadata": {
        "id": "MqqCpHYfDJK0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBU-xSODDLjF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}