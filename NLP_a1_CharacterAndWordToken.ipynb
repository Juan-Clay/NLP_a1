{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVUKOqAaN4LwRiG9DsP4zy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juan-Clay/NLP_a1/blob/main/NLP_a1_CharacterAndWordToken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce8faVx9OcXB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n"
      ],
      "metadata": {
        "id": "kfYc1MuBZ_nW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "BKPyprSIakJn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "print(\"Done1\")"
      ],
      "metadata": {
        "id": "96nEFlGcaqTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92c016a-2b95-4ae1-9fab-bbc1e34768f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Done1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts (e.g., movie reviews).\n",
        "        num_words (int or None): Maximum number of tokens to keep (based on frequency).\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing binary presence of characters.\n",
        "    \"\"\"\n",
        "    # Use texts_to_matrix with mode 'binary' to create fixed-length vectors.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]"
      ],
      "metadata": {
        "id": "XCwxZ7rgfndo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MthdXI4pjIVe",
        "outputId": "b025cc03-f28f-4e67-e8ae-41726fcd6e05"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_char = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer_char.word_index) + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjttiFv6p0YL",
        "outputId": "18af23da-9206-4da2-cb21-f1742d08abb7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocabulary size: 134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer_char, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer_char, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer_char, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n"
      ],
      "metadata": {
        "id": "p-_IeylytqG0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output"
      ],
      "metadata": {
        "id": "rztDu4Nfv3A-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ6Xcvqgv4je",
        "outputId": "702cad2e-ea88-47be-ddcc-1977b6f038ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6708 | Val Loss: 0.6625 | Accuracy: 0.6102 | Precision: 0.5880 | Recall: 0.6547\n",
            "Epoch 02 | Training Loss: 0.6613 | Val Loss: 0.6622 | Accuracy: 0.6034 | Precision: 0.5799 | Recall: 0.6601\n",
            "Epoch 03 | Training Loss: 0.6601 | Val Loss: 0.6603 | Accuracy: 0.6182 | Precision: 0.5885 | Recall: 0.7067\n",
            "Epoch 04 | Training Loss: 0.6569 | Val Loss: 0.6589 | Accuracy: 0.6104 | Precision: 0.5863 | Recall: 0.6671\n",
            "Epoch 05 | Training Loss: 0.6575 | Val Loss: 0.6611 | Accuracy: 0.6080 | Precision: 0.6147 | Recall: 0.5128\n",
            "Epoch 06 | Training Loss: 0.6533 | Val Loss: 0.6605 | Accuracy: 0.6016 | Precision: 0.5721 | Recall: 0.7067\n",
            "Epoch 07 | Training Loss: 0.6509 | Val Loss: 0.6568 | Accuracy: 0.6122 | Precision: 0.5966 | Recall: 0.6180\n",
            "Epoch 08 | Training Loss: 0.6468 | Val Loss: 0.6565 | Accuracy: 0.6102 | Precision: 0.5877 | Recall: 0.6568\n",
            "Epoch 09 | Training Loss: 0.6446 | Val Loss: 0.6584 | Accuracy: 0.6104 | Precision: 0.5985 | Recall: 0.5965\n",
            "Epoch 10 | Training Loss: 0.6420 | Val Loss: 0.6574 | Accuracy: 0.6098 | Precision: 0.5880 | Recall: 0.6518\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6581 | Test Accuracy: 0.6062 | Test Precision: 0.5983 | Test Recall: 0.6464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Changing to Word-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def word_level_tokenizer(texts, num_words=10000):  # Limit vocabulary to 10,000 words\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def word_texts_to_bow(tokenizer, texts):\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='count')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]"
      ],
      "metadata": {
        "id": "wqLL7Cq-xL5q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_word = word_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer_word.word_index) + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZ63Zr625oS",
        "outputId": "0d2fbe29-7c78-4a70-a8ef-6b25a5c46451"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocabulary size: 80169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = word_texts_to_bow(tokenizer_word, train_texts)\n",
        "X_val   = word_texts_to_bow(tokenizer_word, val_texts)\n",
        "X_test  = word_texts_to_bow(tokenizer_word, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n"
      ],
      "metadata": {
        "id": "UcAtDwch9wNc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New input size:\", X_train.shape[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4WBBVw0_zwC",
        "outputId": "86b6c2f5-c8ea-44d5-cb53-c3d72acae8b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New input size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)  #adaptive optimization\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-LBSK95-ac3",
        "outputId": "a83a5cfb-5dc8-42da-eb8f-69d02510ea93"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.4464 | Val Loss: 0.3479 | Accuracy: 0.8596 | Precision: 0.9159 | Recall: 0.7822\n",
            "Epoch 02 | Training Loss: 0.2003 | Val Loss: 0.3259 | Accuracy: 0.8758 | Precision: 0.8303 | Recall: 0.9348\n",
            "Epoch 03 | Training Loss: 0.0914 | Val Loss: 0.3731 | Accuracy: 0.8816 | Precision: 0.8779 | Recall: 0.8779\n",
            "Epoch 04 | Training Loss: 0.0301 | Val Loss: 0.4905 | Accuracy: 0.8744 | Precision: 0.8748 | Recall: 0.8647\n",
            "Epoch 05 | Training Loss: 0.0080 | Val Loss: 0.6071 | Accuracy: 0.8784 | Precision: 0.8709 | Recall: 0.8795\n",
            "Epoch 06 | Training Loss: 0.0021 | Val Loss: 0.6954 | Accuracy: 0.8762 | Precision: 0.8609 | Recall: 0.8882\n",
            "Epoch 07 | Training Loss: 0.0010 | Val Loss: 0.7549 | Accuracy: 0.8750 | Precision: 0.8554 | Recall: 0.8932\n",
            "Epoch 08 | Training Loss: 0.0006 | Val Loss: 0.7950 | Accuracy: 0.8756 | Precision: 0.8556 | Recall: 0.8944\n",
            "Epoch 09 | Training Loss: 0.0004 | Val Loss: 0.8246 | Accuracy: 0.8762 | Precision: 0.8641 | Recall: 0.8837\n",
            "Epoch 10 | Training Loss: 0.0003 | Val Loss: 0.8581 | Accuracy: 0.8756 | Precision: 0.8573 | Recall: 0.8919\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.8927 | Test Accuracy: 0.8626 | Test Precision: 0.8663 | Test Recall: 0.8575\n"
          ]
        }
      ]
    }
  ]
}